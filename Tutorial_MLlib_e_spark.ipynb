{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial para trabalhar com topic modeling usando a biblioteca mllib do spark para aplicações em big data\n",
    "Este tutorial vai apresentar o conceito de topic modeling superficialmente, o foco dele é como utilizar este método de classificação de texto usando a biblioteca mllib do spark. Essa biblioteca, assim como o spark, vai ser introduzida em mais detalhes conforme necessário para a sua aplicação em contextos genéricos de classificação de textos usando topic modeling.\n",
    "\n",
    "![alt text](https://www.depends-on-the-definition.com/wp-content/uploads/2018/11/IntroToLDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é topic modeling e para que serve?\n",
    "Como introduzido, topic modeling é um método de classificação de textos. Topic modeling define tópicos e calcula a aderência de um texto a cada um destes tópicos, o resultado da aderência a tópico é um coeficiente respectivo. \n",
    "\n",
    "Existem várias formas de calcular os coeficientes de aderência de um texto a um conjunto de tópicos, a forma que vai ser usada neste texto é LDA - Latent Dirichlet Allocation. LDA vai ser explicado a seguir no tutorial, ensinar a matemática por trás do algoritmo não é o propósito deste tutorial mas é importante que fique claro como o cálculo LDA é bastante custoso em termos de processamento; ele foi a escolha deste tutorial para classificação de enormes quantidades de texto por que o Spark é uma ferramenta ótima de paralelização de processamento.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Latent Dirichlet... que?\n",
    "Deixa o nome pra lá por enquanto, ela será chamada de 'a coisa' para que o nome não cause confusão. Vamos focar primeiro no fato de 'a coisa' ser um método de classificação de documentos por tópicos, que tem parâmetros fixos e parâmetros latentes (guarda essa palavra!) - um subconjunto dentre os parâmetros latentes é o conjunto dos tópicos, a quantidade de tópicos é arbitrária. Se pensarmos no algoritmo como algo que fornece a probabilidade de um tópico ser o certo para um dado texto, e simplificando as variáveis um pouco, podemos descrevê-lo da seguinte maneira:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "P (&topico&|&documento,&parametros fixos) = Aderencia&do&texto&ao&topico&\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Agora temos um propósito claro, achar o coeficiente de aderência do texto ao topico. Para chegarmos a esse objetivo, talvez seja mais fácil pensar no inverso desta probabilidade, ao invés de tentarmos saber a probabilidade do tópico dado o texto nós vamos tentar achar a probabilidade do texto dado o tópico; é confuso mas talvez isso ajude: vamos tentar gerar o texto que temos a partir de variar o valor das nossas variáveis que medem aderência a tópico e gerando um bloco de texto com elas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dirichlet</h3>\n",
    "\n",
    "Como estamos usando LDA (Latent Dirichlet Allocation), é importante entender um pouco como funciona uma Dirichlet, pelo menos para o nosso caso específico.\n",
    "O que precisamos obter com Dirichlet é a probabilidade de um tópico ser o certo para um dado documento.\n",
    "Vamos quebrar a explicação para facilitar o entendimento: \n",
    "\n",
    "1. Escolher o tópico de onde virá esta palavra neste documento\n",
    "\n",
    "2. Escolher a palavra de dentro do tópico escolhido\n",
    "\n",
    "Vamos lá:\n",
    "\n",
    "1. Escolher o tópico:\n",
    "\n",
    "  *   Para escolher o tópico preciso de: probabilidade de tópico para aquele documento ($\\alpha$)\n",
    "      *   Cada documento tem suas próprias probabilidades do tópico: \"Tópicos do documento\"\n",
    "\n",
    "  \n",
    "  \n",
    "  1. 1  Escolher as probabilidades de tópico no documento:\n",
    "\n",
    "    *   Restrições: vetor meio esparso\n",
    "      *   Obter este vetor de probabilidades como uma amostra de uma distribuição de Dirichlet\n",
    "\n",
    "\n",
    "\n",
    "2. Escolher palavra do tópico:\n",
    "\n",
    "  *   Para escolher o tópico preciso de: probabilidades de palavra por tópico ($\\beta$)\n",
    "\n",
    "\n",
    "  2. 1  Escolher as probabilidades da palavra no tópico($\\varphi$):\n",
    "\n",
    "    *   Restrições: vetor meio esparso\n",
    "      *   Obter este vetor de probabilidades como uma amostra de uma distribuição de Dirichlet\n",
    "\n",
    "\n",
    "\n",
    "3. Escolhido o tópico, escolher a palavra:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/635/1*qwA4jyRFBB6Htn3X4aftSw.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que é spark e mllib?\n",
    "Uma busca rápida no google já te leva a se perder no mar de termos, \"*You might already know Apache Spark as a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing*\", na verdade não, se você está aqui você provavelmente não conhece spark ou mllib, então vamos nos introduzir a essas duas coisas. Spark é, afinal, uma engine de big data processing... o que quer dizer que Spark tem seu próprio método para distribuir o processamento de uma tarefa para várias maquinas com o intuito de reduzir o tempo que leva para processar conjuntos muito grandes de dados (big data). A gente precisa do spark, porque às vezes a análise de um problema necessita o processamento de dados, muitos e muitos dados, mais dados do que uma máquina sozinha seria capaz de processar. Então, posto que temos tanta informação para processar, vamos utilizar uma aplicação distribuida - spark - que consiga dividir esse bloco de trabalho enorme em bloquinhos menores e fazer máquinas diferentes processarem cada bloquinho para depois obter o mesmo resultado em menos tempo.\n",
    "\n",
    "![alt text](https://dzone.com/storage/temp/9507196-data-flow2x-768x897.png)\n",
    "\n",
    "Mllib é a biblioteca de machine learning do spark, ela vai permitir o uso LDA de forma otimizada para o processamento distribuido que o spark oferece. No tutorial, nós vamos aplicar ela localmente, mas a transição de local para em um cluster na AWS é extremamente fácil e vai ser mostrada aqui também."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Por que usar eles ao invés de scikit-learn ou outra biblioteca de machine learning?\n",
    "\n",
    "Vamos responder esta com uma atividade, no final deste tutorial, onde você vai tentar fazer algo parecido com o que estamos prestes a fazer com o spark e mllib, mas com uma biblioteca não distribuida - o scikit-learn; o objetivo desta atividade é mostrar que para volumes massivos de informação, é muito mais adequado utilizar uma aplicação distribuida como o spark, uma vez que o tempo para processar essa quantidade de dados em uma máquina se torna inadequado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seria prudente ter uma noção sólida dos seguintes conceitos para passar deste ponto:\n",
    "\n",
    "*   Programação em Python\n",
    "*   Topic modeling (nada demais, só ter certeza que o conteúdo do tutorial até agora ficou claro)\n",
    "\n",
    "## Para começar, você vai precisar ter acesso a:\n",
    "\n",
    "*   Uma máquina Ubuntu (O tutorial não foi testado em outros OS's)\n",
    "\n",
    "#### E nessa maquina, vai ser necessário ter instalado:\n",
    "*   Python\n",
    "*   Pip\n",
    "*   Java (recomendamos a versão 8 do jdk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como instalar algumas partes que talvez você não tenha\n",
    "Supondo que você não tenha já o pyspark e o findspark, segue como instalar eles - para usar spark pelo python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pyspark\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes de começar, vamos importar tudo que vamos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import pyspark\n",
    "import string\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uau, parece um monte de coisa, e é! Tudo isso é necessario para que seja possivel o uso do spark via python - e no findspark init, você talvez tenha um diretorio diferente onde você instalou o spark e é esse repositorio que devia vir aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando, vamos iniciar uma seção spark e ajustar tudo agora que temos as bibliotecas importadas\n",
    "\n",
    "Essa é a porta de entrada para desenvolvimento em Spark, aqui a gente inicia a seção Spark em cima da qual todo o resto da aplicação vai rodar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName = \"LDA_app\")\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "dataset_location = 'songdata.csv'\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Começando, vamos obter dados\n",
    "Nós vamos usar dados deste db, contendo as letras de musicas:\n",
    "\n",
    "https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics\n",
    "\n",
    "(~100MB)\n",
    "\n",
    "Com os dados em mãos, precisamos alimentar eles ao Spark, da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler os dados do arquivo \n",
    "data_df = spark.read.csv(dataset_location,header=True, multiLine=True,sep=\",\");\n",
    "\n",
    "# Excluir as linhas nulas, que não contem dados\n",
    "data_df = data_df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa, nós estamos pegando os dados usando a spark.read.csv, os parametros header e multiline indicam que a primeira linha de dados é na verdade a linha que da nome às colunas e que a informação da tabela pode ter quebras de linha, famosos \"\\n\", respectivamente. Nós utilizamos tambem o parametro sep, para indicar que a informação ta separada por uma virgula no arquivo .csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados obtidos, vamos agora \n",
    "Tokenizar os dados. Mais uma palavra chave, tokenizar, que nada mais significa do que aglutinar tudo aquilo que é parecido - como estamos trabalhando com letras de musicas, tem muita abreviação, palavras escritas de forma estilizada e coisas do tipo, mas só nos interessa uma variação destas, contanto que ela tenha significado sintatico diferente das outras palavras. Por exemplo, \"feel\", \"feeling\", \"feels\" vão se tornar uma palavra só."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizando as palavras da coluna texto e \n",
    "# adicionando uma coluna palavras para guardar essa informação\n",
    "tokenizer = Tokenizer(inputCol=\"text\\r\", outputCol=\"words\")\n",
    "wordsDataFrame = tokenizer.transform(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora dando uma limpada nos dados\n",
    "Vamos tirar as 20 palavras mais recorrentes, as que tem caracteres numericos e as com menos de 3 caracteres, para isso, vai ser necessario executarmos alguns passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essas duas linhas agrupam as palavras que nós não queremos\n",
    "cv_tmp = CountVectorizer(inputCol=\"words\", outputCol=\"tmp_vectors\")\n",
    "cv_tmp_model = cv_tmp.fit(wordsDataFrame)\n",
    "\n",
    "# Aqui vão ficar as nossas 20 palavras mais fraquentes\n",
    "top20 = list(cv_tmp_model.vocabulary[0:20])\n",
    "\n",
    "# Aqui vao ficar as que tem menos de 3 caracteres\n",
    "more_then_3_charachters = [word for word in cv_tmp_model.vocabulary if len(word) <= 3]\n",
    "\n",
    "# Aqui vão ficar as que tem digitos numericos\n",
    "contains_digits = [word for word in cv_tmp_model.vocabulary if any(char.isdigit() for char in word)]\n",
    "\n",
    "#\n",
    "# Voce pode adicionar palavras que deseja filtrar nesta lista!\n",
    "#\n",
    "stopwords = [] \n",
    "\n",
    "# Juntando as 3 listas de coisas que não queremos\n",
    "stopwords = stopwords + top20 + more_then_3_charachters + contains_digits\n",
    "\n",
    "# Podemos remover as palavras que não queremos, finalmente\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords = stopwords)\n",
    "wordsDataFrame = remover.transform(wordsDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dando uns passos para trás\n",
    "\n",
    "Vamos repetir o processo de agrupar as palavras, agora que filtramos as que não eram de interesse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new CountVectorizer model without the stopwords\n",
    "Vector = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "model = Vector.fit(wordsDataFrame)\n",
    "result = model.transform(wordsDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alguem falou burocracia??\n",
    "É, até no codigo tem burocracia... Precisamos formatar a informação que temos para ela ser palatavel para o mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionando um id unico para cada item (a funcao não faz isso automaticamente, mas como só vamos fazer uma vez, sabemos que será unico)\n",
    "# Alem disso, estamos tambem pegando só a informação que queremos, e com a ajuda da funcao, temos ela na ordem [id's, vetores]\n",
    "sparsevector = result.withColumn(\"id\", monotonically_increasing_id()).select('id', 'vectors')\n",
    "\n",
    "# Aqui convertemos o que temos para algo que o mllib consegue usar\n",
    "sparsevector = MLUtils.convertMatrixColumnsFromML(sparsevector)\n",
    "\n",
    "corpus = sparsevector.select(\"id\", \"vectors\").rdd.map(lambda x: [x[0], Vectors.fromML(x[1])]).cache()# , lambda y: Vectors.fromML(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cream of the crop\n",
    "Aqui a gente treina nosso modelo LDA! Poderiamos ter escolhido varios parametros, mas deixamos ele nos defaults que são aceitaveis nesse caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModel = LDA.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, os resultados!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic nr: 0\n",
      "it's 0.013097001561420267\n",
      "know 0.01257571160062658\n",
      "don't 0.010342279839013244\n",
      "want 0.010165494147367896\n",
      "come 0.010013229548783729\n",
      "down 0.009014945958699836\n",
      "little 0.0087460041975338\n",
      "your 0.008643456614179191\n",
      "just 0.008257074476161756\n",
      "time 0.00804964932682644\n",
      "never 0.007839994766036675\n",
      "this 0.007406852115895891\n",
      "what 0.006952366689833028\n",
      "right 0.006931370223130331\n",
      "with 0.006675356052250044\n",
      "topic nr: 1\n",
      "know 0.012904227623244986\n",
      "it's 0.012795998907416076\n",
      "don't 0.01224283814250415\n",
      "down 0.009811120551071284\n",
      "come 0.009516192723962133\n",
      "your 0.009434573085638298\n",
      "just 0.009094153942955105\n",
      "this 0.008223504709406453\n",
      "little 0.007921326686503303\n",
      "want 0.007876567487268137\n",
      "time 0.007657289239438381\n",
      "never 0.007416029123822786\n",
      "what 0.0071675874379312785\n",
      "that 0.006931633659937003\n",
      "right 0.006869325570946568\n",
      "topic nr: 2\n",
      "don't 0.012497995403132124\n",
      "know 0.012136384569527935\n",
      "it's 0.011909378738007868\n",
      "down 0.011011645354008183\n",
      "want 0.010448598764137682\n",
      "never 0.008534524489109862\n",
      "just 0.00848545517253239\n",
      "your 0.008400785322532765\n",
      "come 0.007994359415753948\n",
      "this 0.007452431740223357\n",
      "little 0.00735404038429189\n",
      "what 0.0072972446321528895\n",
      "time 0.006926529223286466\n",
      "that 0.006724620173839327\n",
      "right 0.006629020884842827\n",
      "topic nr: 3\n",
      "know 0.013191086599147087\n",
      "don't 0.012724908111718987\n",
      "it's 0.012331806294430115\n",
      "down 0.009839752997329877\n",
      "time 0.009355532879569492\n",
      "come 0.009293985545407472\n",
      "your 0.009038847775048403\n",
      "want 0.008785285585292043\n",
      "just 0.008313967739014145\n",
      "this 0.007856892116873257\n",
      "what 0.00763301463853014\n",
      "little 0.007553877972057734\n",
      "never 0.007529914429575983\n",
      "right 0.0064480302241020785\n",
      "that 0.0058463347279782245\n",
      "topic nr: 4\n",
      "know 0.013847236382122756\n",
      "it's 0.012733259306734824\n",
      "don't 0.010910815769181787\n",
      "come 0.009827518288455375\n",
      "down 0.00971431142437109\n",
      "your 0.009466079831098605\n",
      "little 0.008903117723442688\n",
      "just 0.008840822949978237\n",
      "want 0.008811765593794759\n",
      "never 0.008195868815860994\n",
      "what 0.007823069157750063\n",
      "time 0.007708699119261823\n",
      "this 0.007360971800198759\n",
      "that 0.006767348892847061\n",
      "right 0.006651183350242148\n",
      "topic nr: 5\n",
      "it's 0.012959640740582257\n",
      "know 0.012454135760355575\n",
      "don't 0.012065361339115734\n",
      "down 0.010372482313407708\n",
      "just 0.008861620409256487\n",
      "time 0.008720749718195421\n",
      "want 0.00864691684695829\n",
      "never 0.008467696500330926\n",
      "your 0.008272491554184387\n",
      "that 0.007965332956121271\n",
      "come 0.007919922742457003\n",
      "little 0.007759744822390523\n",
      "what 0.007422826643369978\n",
      "right 0.007169064065669407\n",
      "this 0.007137700456451898\n",
      "topic nr: 6\n",
      "it's 0.01177028780826831\n",
      "don't 0.011653027991611326\n",
      "know 0.011523884734615299\n",
      "your 0.009204496825953947\n",
      "come 0.009165457614454137\n",
      "little 0.008969119983823411\n",
      "down 0.008723162878755304\n",
      "want 0.008615183311155261\n",
      "never 0.008545601322853433\n",
      "just 0.008441437901168105\n",
      "time 0.007805307229111884\n",
      "that 0.007616812222936119\n",
      "right 0.006955607652557034\n",
      "this 0.006925182276024013\n",
      "what 0.006314570174512563\n",
      "topic nr: 7\n",
      "know 0.013516697722940174\n",
      "it's 0.013120220670470981\n",
      "don't 0.011227261497060072\n",
      "down 0.009087588959007483\n",
      "time 0.009086455838787219\n",
      "want 0.009037105613112774\n",
      "little 0.008556605317935385\n",
      "come 0.008062340893648776\n",
      "your 0.00792980183618197\n",
      "just 0.007890423054379456\n",
      "never 0.007889806588357227\n",
      "what 0.0072186781019190896\n",
      "this 0.007080402556751736\n",
      "that 0.006891567975608211\n",
      "right 0.006666891545694479\n",
      "topic nr: 8\n",
      "it's 0.012339461598112597\n",
      "don't 0.01191210356995804\n",
      "know 0.011306794482172528\n",
      "little 0.011020530360081153\n",
      "down 0.009664634174350352\n",
      "never 0.009429021874929753\n",
      "just 0.00919748704230158\n",
      "want 0.00883940864820124\n",
      "come 0.008525084729142965\n",
      "time 0.008431435810123455\n",
      "your 0.008329059429693954\n",
      "that 0.007565710813802625\n",
      "what 0.007161814505436151\n",
      "this 0.006897775749727561\n",
      "like 0.006338749707241153\n",
      "topic nr: 9\n",
      "know 0.012974650868482419\n",
      "don't 0.01213464246977153\n",
      "it's 0.011193190918665117\n",
      "down 0.009763094795732329\n",
      "time 0.009092041812186456\n",
      "come 0.009054955174586073\n",
      "never 0.008985050202144799\n",
      "just 0.008720406872184163\n",
      "little 0.008227870105893285\n",
      "want 0.008147726896972695\n",
      "this 0.007772699256838655\n",
      "your 0.007383724283564372\n",
      "with 0.007077208739230123\n",
      "right 0.006988537460269136\n",
      "what 0.006943241783789989\n"
     ]
    }
   ],
   "source": [
    "topics = ldaModel.describeTopics(maxTermsPerTopic = 15)\n",
    "for x, topic in enumerate(topics):\n",
    "    print ('topic nr: ' + str(x))\n",
    "    words = topic[0]\n",
    "    weights = topic[1]\n",
    "    for n in range(len(words)):\n",
    "        print (model.vocabulary[words[n]] + ' ' + str(weights[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
