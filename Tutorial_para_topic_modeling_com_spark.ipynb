{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Tutorial para topic modeling com spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDYLx-9_kbxV",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial para trabalhar com topic modeling usando a biblioteca mllib do spark para aplicações em big data\n",
        "Este tutorial vai apresentar o conceito de topic modeling superficialmente, o foco dele é como utilizar este método de classificação de texto usando a biblioteca mllib do spark. Essa biblioteca, assim como o spark, vai ser introduzida em mais detalhes conforme necessário para a sua aplicação em contextos genéricos de classificação de textos usando topic modeling.\n",
        "\n",
        "![alt text](https://www.depends-on-the-definition.com/wp-content/uploads/2018/11/IntroToLDA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S_4ppvNkbxX",
        "colab_type": "text"
      },
      "source": [
        "## O que é topic modeling e para que serve?\n",
        "Como introduzido, topic modeling é um método de classificação de textos. Topic modeling define tópicos e calcula a aderência de um texto a cada um destes tópicos, o resultado da aderência a tópico é um coeficiente respectivo. \n",
        "\n",
        "Existem várias formas de calcular os coeficientes de aderência de um texto a um conjunto de tópicos, a forma que vai ser usada neste texto é LDA - Latent Dirichlet Allocation. LDA vai ser explicado a seguir no tutorial, ensinar a matemática por trás do algoritmo não é o propósito deste tutorial mas é importante que fique claro como o cálculo LDA é bastante custoso em termos de processamento; ele foi a escolha deste tutorial para classificação de enormes quantidades de texto por que o Spark é uma ferramenta ótima de paralelização de processamento.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Latent Dirichlet... que?\n",
        "Deixa o nome pra lá por enquanto, ela será chamada de 'a coisa' para que o nome não cause confusão. Vamos focar primeiro no fato de 'a coisa' ser um método de classificação de documentos por tópicos, que tem parâmetros fixos e parâmetros latentes (guarda essa palavra!) - um subconjunto dentre os parâmetros latentes é o conjunto dos tópicos, a quantidade de tópicos é arbitraria. Se pensarmos no algoritmo como algo que fornece a probabilidade de um tópico ser o certo para um dado texto, e simplificando as variaveis um pouco, podemos descrevê-lo da seguinte maneira:\n",
        "\n",
        "$$\\begin{eqnarray}\n",
        "P (&topico&|&documento,&parametros fixos) = Aderencia&do&texto&ao&topico&\n",
        "\\end{eqnarray}$$\n",
        "\n",
        "Agora temos um proposito claro, achar o coeficiente de aderencia do texto ao topico. Para chegarmos a esse objetivo, talvez seja mais facil pensar no invérso desta probabilidade, ao invés de tentarmos saber a probabilidade do topico dado o texto nós vamos tentar achar a probabilidade do texto dado o topico; é confuso mas talvez isso ajude: vamos tentar gerar o texto que temos a partir de variar o valor das nossas variaveis que medem aderencia a topico e gerando um bloco de texto com elas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARY5npih4prg",
        "colab_type": "text"
      },
      "source": [
        "<h3>Dirichlet</h3>\n",
        "\n",
        "Como estamos usando LDA (Latent Dirichlet Allocation), é importante entender um pouco como funciona uma Dirichlet, pelo menos para o nosso caso específico.\n",
        "O que precisamos obter com Dirichlet é a probabilidade de um tópico ser o certo para um dado documento.\n",
        "Vamos quebrar a explicação para facilitar o entendimento: \n",
        "\n",
        "1. escolher o tópico de onde virá esta palavra neste documento\n",
        "\n",
        "2. escolher a palavra de dentro do tópico escolhido\n",
        "\n",
        "Coisa do Ayres:\n",
        "\n",
        "1. escolher o tópico:\n",
        "\n",
        "  *   para escolher o tópico preciso de: probabilidade de tópico para aquele documento\n",
        "    *   Cada documento tem suas próprias probabilidades do tópico\n",
        "        *   \"Tópicos do documento\"\n",
        "\n",
        "  \n",
        "  \n",
        "  1. 1  Escolher as probabilidades de tópico no documento:\n",
        "\n",
        "    *   restrições: vetor meio esparso\n",
        "      *   obter este vetor de probabilidades como uma amostra de uma distribuição de Dirichlet\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/635/1*qwA4jyRFBB6Htn3X4aftSw.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. escolher palavra do tópico:\n",
        "\n",
        "  *   tenho um vetor de probabilidades de palavra para aquele tópico\n",
        "\n",
        "  \n",
        "  \n",
        "  2. 1  Preciso de probabilidades de palavra por tópico:\n",
        "\n",
        "    *   restrições: vetor meio esparso\n",
        "      *   obter este vetor de probabilidades como uma amostra de uma distribuição de Dirichlet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Escolhido o tópico, escolher a palavra:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcne-OCikbxY",
        "colab_type": "text"
      },
      "source": [
        "## O que é spark e mllib?\n",
        "Uma busca rapida no google e você se perde no mar de termos, \"You might already know Apache Spark as a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing\", na verdade não, se você ta aqui você não conhece spark e mllib, então vamos nos introduzir a essas duas coisas. Spark é, afinal, uma engine de big data processing... significando que Spark tem seu proprio metodo para distribuir o processamento de uma tarefa para varias maquinas com o intuito de reduzir o tempo que leva para processar big data. A gente precisa de spark, por que as vezes a analize de um problema necessita o processamento de dados, muitos e muitos dados, mais dados do que uma maquina seria capaz de processar. Então, posto que temos tanta informação para processar, vamos utilizar uma aplicação distribuida - spark - que consiga dividir esse bloco de trabalho enorme em bloquinhos menores e fazer maquinas diferentes processarem cada bloquinho para depois obter o mesmo resultado em menos tempo.\n",
        "\n",
        "![alt text](https://dzone.com/storage/temp/9507196-data-flow2x-768x897.png)\n",
        "\n",
        "Mllib é a biblioteca de machine learning do spark, ela vai permitir que a gente use LDA de forma otimizada para processamento distribuido que o spark oferece. No tutorial, nós vamos aplicar ela localmente, mas a transição de local para em um cluster na AWS é extremamente facil e vai ser mostrada aqui tambem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyTnnb5EkbxY",
        "colab_type": "text"
      },
      "source": [
        "## Para que usar eles ao invés de scikit-learn ou outra biblioteca de machine learning?\n",
        "\n",
        "Vamos responder esta com uma atividade, no final deste tutorial, onde você vai tentar fazer algo parecido com o que a gente esta prestes a fazer com o spark e mllib, mas com uma biblioteca não distribuida - o scikit-learn; o objetivo desta atividade é ser mostrar que para volumes massivos de informação, é muito mais adequado utilizar uma aplicação distribuida como o spark, uma vez que o tempo para processar essa quantidade de dados em uma maquina se torna inadequado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDXq0LJSkbxZ",
        "colab_type": "text"
      },
      "source": [
        "## Seria prudente ter uma noção solida dos seguintes conceitos para passar deste ponto:\n",
        "\n",
        "*   Programação em Python\n",
        "*   Topic modeling (nada demais, só ter certeza que o conteudo do tutorial até agora ficou claro)\n",
        "\n",
        "## Para começar, você vai precisar ter acesso a:\n",
        "\n",
        "*   Uma maquina Ubuntu (O tutorial não foi testado em outros OS's)\n",
        "\n",
        "#### E nessa maquina, vai ser necessario ter instalado:\n",
        "*   Python\n",
        "*   Pip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF3FD7pRkbxa",
        "colab_type": "text"
      },
      "source": [
        "## Como instalar algumas partes que talvez você não tenha\n",
        "Supondo que você não tenha já o pyspark, segue como instalar o ele - para usar spark pelo python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iLqLJtBAFkJ",
        "colab_type": "code",
        "outputId": "bdf0ad28-be9f-46a6-90dc-a530c5f7c118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 59kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 51.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=63ff35bf70eb2767a6e3e4f5ccb6c396c5e0640cbec5b8cddd30017b09a3ce64\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipY6DczzZ1tI",
        "colab_type": "text"
      },
      "source": [
        "## Começando, vamos iniciar uma seção spark\n",
        "Essa é a porta de entrada para desenvolvimento em Spark, aqui a gente inicia a seção Spark em cima da qual todo o resto da aplicação vai rodar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hG_Auj7aHpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6wixO7kkbxa",
        "colab_type": "text"
      },
      "source": [
        "## Começando, vamos obter dados\n",
        "Nós vamos usar dados deste db, contendo as letras de musicas:\n",
        "\n",
        "https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics\n",
        "\n",
        "(~100MB)\n",
        "\n",
        "Agora precisamos importar as letras para esse notebook, utilizando o seguinte comando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd55ClrzjlNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "1bb969ca-2a99-4ca2-8a95-e3088030eb40"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c692f215-7e9c-4687-b540-47e9f09581a6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c692f215-7e9c-4687-b540-47e9f09581a6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-21dc3c638f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peOjiDbhkbxb",
        "colab_type": "text"
      },
      "source": [
        "## Dados obtidos, vamos agora "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXEg0dXRkYID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}